# CS231n: Deep Learning for Computer Vision
## Lecture 1: Introduction (Part 1)

**Instructors:** Fei-Fei Li, Ehsan Adeli, Zane Durante
**Date:** April 1, 2025

---

## Slide 1: Title

# CS231n: Deep Learning for Computer Vision
## Lecture 1: Introduction

---

## Slide 2-3: Welcome to CS231n

Course history spanning 2015-2023 with various instructors and teaching assistants.

---

## Slide 4: CS231n's Ten Year Anniversary: Returning Lecturers

| Lecturer | Years |
|----------|-------|
| Fei-Fei Li | 2015-2025 |
| Andrej Karpathy | 2015-2016 |
| Justin Johnson | 2016-2019, 2025 |
| Serena Yeung | 2017-2019 |
| Ranjay Krishna | 2020-2021 |
| Danfei Xu | 2020-2021 |
| Jiajun Wu | 2022 |
| Ruohan Gao | 2022-2023 |
| Yunzhu Li | 2023 |
| Ehsan Adeli | 2024-2025 |
| Zane Durante | 2024-2025 |

---

## Slides 5-12: Field Relationships Venn Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ARTIFICIAL INTELLIGENCE                          â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚    â”‚                                                              â”‚     â”‚
â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚
â”‚    â”‚  â”‚                  â”‚      â”‚      MACHINE LEARNING       â”‚  â”‚     â”‚
â”‚    â”‚  â”‚   COMPUTER       â”‚      â”‚                             â”‚  â”‚     â”‚
â”‚    â”‚  â”‚   VISION         â”‚      â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚  â”‚     â”‚
â”‚    â”‚  â”‚                  â”‚      â”‚    â”‚                 â”‚      â”‚  â”‚     â”‚
â”‚    â”‚  â”‚              â”Œâ”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤  DEEP LEARNING  â”‚      â”‚  â”‚     â”‚
â”‚    â”‚  â”‚              â”‚###â”‚      â”‚    â”‚                 â”‚      â”‚  â”‚     â”‚
â”‚    â”‚  â”‚              â”‚###â”‚ <â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”‚  THIS CLASS     â”‚      â”‚  â”‚     â”‚
â”‚    â”‚  â”‚              â”‚###â”‚      â”‚    â”‚                 â”‚      â”‚  â”‚     â”‚
â”‚    â”‚  â”‚              â””â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤                 â”‚      â”‚  â”‚     â”‚
â”‚    â”‚  â”‚                  â”‚      â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚  â”‚     â”‚
â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚                             â”‚  â”‚     â”‚
â”‚    â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚
â”‚    â”‚                                                              â”‚     â”‚
â”‚    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚     â”‚
â”‚    â”‚     â”‚  NATURAL LANGUAGE   â”‚    â”‚      SPEECH        â”‚       â”‚     â”‚
â”‚    â”‚     â”‚    PROCESSING       â”‚    â”‚    RECOGNITION     â”‚       â”‚     â”‚
â”‚    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚     â”‚
â”‚    â”‚                                                              â”‚     â”‚
â”‚    â”‚  ROBOTICS                                                    â”‚     â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                         â”‚
â”‚  Related Fields: Mathematics, Neuroscience, Computer Science,           â”‚
â”‚                  Physics, Biology, Psychology                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

The ### shaded region represents "This class" - the intersection of
Computer Vision and Deep Learning within Machine Learning within AI.
```

---

## Slide 13: Today's Agenda

- A brief history of computer vision and deep learning
- CS231n overview

---

## Slide 14: Evolution's Big Bang

**Cambrian Explosion: 530-540 million years B.C.**

The rapid diversification of life forms, including the evolution of eyes.

---

## Slide 15: Eyes Across Species

[Images showing diversity of eyes: octopus, insect compound eyes, chameleon, human baby]

---

## Slide 16: Camera Obscura

```
CAMERA OBSCURA PRINCIPLE (Leonardo da Vinci, 16th Century)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

        Object                      Inverted Image
          â—                              â—‹
           \                            /
            \     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          /
             \    â”‚         â”‚         /
              \   â”‚  tiny   â”‚        /
               \  â”‚  hole   â”‚       /
                \ â”‚    â—‹    â”‚      /
                 \â”‚         â”‚     /
                  â”‚         â”‚    /
                  â”‚         â”‚   /
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  /
                              â—‹
                        Projected Image

Light passes through a small aperture, projecting an inverted
image on the opposite surface.

Historical Sources:
- Gemma Frisius, 1545
- Leonardo da Vinci, 16th Century AD
- Encyclopedia, 18th Century
```

---

## Slide 17: Computer Vision is Everywhere!

Applications shown:
- Photographers and cameras
- Drones
- Smartphones (face detection)
- Security cameras
- Mars rovers
- Underwater exploration
- Google Glass

---

## Slide 18: Where did we come from?

*Transition to history section*

---

## Slide 19: Hubel and Wiesel, 1959

```
HUBEL & WIESEL'S VISUAL CORTEX EXPERIMENTS (1959)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Experimental Setup:
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚   STIMULUS SCREEN   â”‚
    â”‚   CAT   â”‚                     â”‚                     â”‚
    â”‚  â”Œâ”€â”€â”€â”  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚        /            â”‚
    â”‚  â”‚ â— â”‚  â”‚  Measures brain     â”‚       /             â”‚
    â”‚  â””â”€â”€â”€â”˜  â”‚  activity           â”‚      / (oriented    â”‚
    â”‚ (brain) â”‚                     â”‚     /   bar)        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Neural Response Patterns:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

SIMPLE CELLS: Response to specific rotation and orientation
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                            â”‚
â”‚  Stimulus:    |    â€”    /    \    â—    â—¯                  â”‚
â”‚               â†‘    â†‘    â†‘    â†‘    â†‘    â†‘                  â”‚
â”‚  Response: [HIGH][MED][MED][LOW][LOW][NONE]               â”‚
â”‚                                                            â”‚
â”‚  Each simple cell responds maximally to a specific         â”‚
â”‚  orientation of edges or lines                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

COMPLEX CELLS: Response to light orientation and movement,
               with some translation invariance

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                            â”‚
â”‚  Position 1:  â•²         Position 2:    â•²                  â”‚
â”‚                 responds                  responds         â”‚
â”‚                                                            â”‚
â”‚  Same orientation at different positions â†’ similar         â”‚
â”‚  response (translation invariance)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Discovery:** The visual cortex has a hierarchical organization with simple and complex cells that detect oriented edges and bars.

---

## Slide 20: Larry Roberts, 1963

```
MACHINE PERCEPTION OF THREE-DIMENSIONAL SOLIDS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

(a) Original Picture     (b) Differentiated      (c) Feature Points
                              Picture                 Selected

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â•³â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•³
   /â”‚          /â”‚          /â”‚          /â”‚         /â”‚          /â”‚
  / â”‚         / â”‚         / â”‚         / â”‚        â•³ â”‚         â•³ â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚       â•³â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•³  â•³
 â”‚           â”‚  â”‚        â”‚           â”‚  â”‚       â”‚â•³          â”‚â•³ â”‚
 â”‚   3D      â”‚  â”‚        â”‚  edge     â”‚  â”‚       â”‚ â•³   â•³â•³   â”‚ â•³ â”‚
 â”‚   solid   â”‚ /         â”‚  lines    â”‚ /        â”‚  â•³ â•³ â•³   â”‚  â•³
 â”‚           â”‚/          â”‚           â”‚/         â”‚   â•³   â•³  â”‚  /
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â•³â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•³/

Roberts' PhD thesis: First work on extracting 3D structure
from 2D images using edge detection and geometric reasoning.
```

---

## Slide 21: The Summer Vision Project (MIT, 1966)

> "The summer vision project is an attempt to use our summer workers
> effectively in the construction of a significant part of a visual system.
> The particular task was chosen partly because it can be segmented into
> sub-problems which will allow individuals to work independently and yet
> participate in the construction of a system complex enough to be a real
> landmark in the development of 'pattern recognition'."
>
> â€” Seymour Papert, MIT, July 7, 1966

---

## Slide 22: David Marr's Vision Theory (1970s)

```
STAGES OF VISUAL REPRESENTATION (David Marr, 1970s)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   INPUT     â”‚    â”‚   PRIMAL    â”‚    â”‚   2Â½-D      â”‚    â”‚    3-D      â”‚
â”‚   IMAGE     â”‚â”€â”€â”€â–ºâ”‚   SKETCH    â”‚â”€â”€â”€â–ºâ”‚   SKETCH    â”‚â”€â”€â”€â–ºâ”‚   MODEL     â”‚
â”‚             â”‚    â”‚             â”‚    â”‚             â”‚    â”‚             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚             â”‚    â”‚ Zero        â”‚    â”‚ Local       â”‚    â”‚ 3-D models  â”‚
â”‚ Perceived   â”‚    â”‚ crossings,  â”‚    â”‚ surface     â”‚    â”‚ hierarchi-  â”‚
â”‚ intensities â”‚    â”‚ blobs,      â”‚    â”‚ orientation â”‚    â”‚ cally       â”‚
â”‚             â”‚    â”‚ edges,      â”‚    â”‚ and discon- â”‚    â”‚ organized   â”‚
â”‚             â”‚    â”‚ bars, ends, â”‚    â”‚ tinuities   â”‚    â”‚ in terms    â”‚
â”‚             â”‚    â”‚ virtual     â”‚    â”‚ in depth    â”‚    â”‚ of surface  â”‚
â”‚             â”‚    â”‚ lines,      â”‚    â”‚ and surface â”‚    â”‚ and         â”‚
â”‚             â”‚    â”‚ groups,     â”‚    â”‚ orientation â”‚    â”‚ volumetric  â”‚
â”‚             â”‚    â”‚ curves,     â”‚    â”‚             â”‚    â”‚ primitives  â”‚
â”‚             â”‚    â”‚ boundaries  â”‚    â”‚             â”‚    â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Example: Basketball

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸ€        â”‚    â”‚   â•­â”€â”€â”€â•®     â”‚    â”‚   â—         â”‚    â”‚     â—       â”‚
â”‚  (photo)    â”‚â”€â”€â”€â–ºâ”‚   â”‚   â”‚     â”‚â”€â”€â”€â–ºâ”‚  (depth)    â”‚â”€â”€â”€â–ºâ”‚   (sphere)  â”‚
â”‚             â”‚    â”‚   â•°â”€â”€â”€â•¯     â”‚    â”‚             â”‚    â”‚             â”‚
â”‚             â”‚    â”‚  (edges)    â”‚    â”‚             â”‚    â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Slide 23: Recognition via Parts (1970s)

```
GENERALIZED CYLINDERS              PICTORIAL STRUCTURES
(Brooks & Binford, 1979)          (Fischler & Elshlager, 1973)

        â”Œâ”€â”€â”€â”                              â—‹
        â”‚   â”‚  â† head                     /â”‚\
        â””â”€â”€â”€â”˜                            / â”‚ \
       â”Œâ”€â”€â”€â”€â”€â”                          â—‹â”€â”€â—‹â”€â”€â—‹
       â”‚     â”‚  â† torso                    â”‚
       â”‚     â”‚                          â—‹â”€â”€â”¼â”€â”€â—‹
       â””â”€â”€â”€â”€â”€â”˜                         /   â”‚   \
      â”Œâ”€â”   â”Œâ”€â”                       â—‹    â”‚    â—‹
      â”‚ â”‚   â”‚ â”‚  â† arms                    â”‚
      â””â”€â”˜   â””â”€â”˜                           â—‹
      â”Œâ”€â”   â”Œâ”€â”                          / \
      â”‚ â”‚   â”‚ â”‚  â† legs                 â—‹   â—‹
      â”‚ â”‚   â”‚ â”‚
      â””â”€â”˜   â””â”€â”˜

Objects represented as          Objects as graph structures
collections of 3D volumetric    with parts (nodes) and
primitives (cylinders, cones)   spatial relationships (edges)
```

---

## Slide 24: Recognition via Edge Detection (1980s)

**John Canny, 1986** - Canny Edge Detector
**David Lowe, 1987**

```
INPUT IMAGE                    EDGE DETECTION OUTPUT

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”Œâ”€â” â”Œâ”€â” â”Œâ”€â”     â”‚         â”‚  â”Œâ”€â” â”Œâ”€â” â”Œâ”€â”     â”‚
â”‚  â”‚ â”‚ â”‚ â”‚ â”‚ â”‚     â”‚   â”€â”€â–º   â”‚  â”‚ â”‚ â”‚ â”‚ â”‚ â”‚     â”‚
â”‚  â”‚ â”‚ â”‚ â”‚ â”‚ â”‚     â”‚         â”‚  â”‚ â”‚ â”‚ â”‚ â”‚ â”‚     â”‚
â”‚  â””â”€â”˜ â””â”€â”˜ â””â”€â”˜     â”‚         â”‚  â””â”€â”˜ â””â”€â”˜ â””â”€â”˜     â”‚
â”‚    Razors        â”‚         â”‚    Edge outlines  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Slide 25: Arriving at an "AI Winter"

- Enthusiasm (and funding!) for AI research dwindled
- "Expert Systems" failed to deliver on their promises
- But subfields of AI continued to grow:
  - Computer vision, NLP, robotics, computational biology, etc.

---

## Slides 26-31: Cognitive and Neuroscience Work

### Biederman (1972): Perceiving Real-World Scenes
Scene context helps object recognition - jumbled scenes are harder to parse.

### Potter (1970s): Rapid Serial Visual Perception (RSVP)
Humans can categorize images shown for very brief durations.

### Thorpe et al. (1996): Speed of Processing
Human visual system can detect animals in images in ~150ms!

### Kanwisher et al. (1997), Epstein & Kanwisher (1998)
Neural correlates of object and scene recognition:
- Fusiform Face Area (FFA) responds to faces
- Parahippocampal Place Area (PPA) responds to scenes/places

---

## Slide 32: Recognition via Grouping (1990s)

**Normalized Cuts** - Shi and Malik, 1997

```
IMAGE SEGMENTATION USING GRAPH CUTS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Original Image          Segmented Result
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”‚     â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚     â”‚ MONK  â”‚   â”‚     â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ”‚ MONK  â”‚â–ˆâ–ˆâ–ˆâ”‚
â”‚     â”‚       â”‚   â”‚     â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ”‚       â”‚â–ˆâ–ˆâ–ˆâ”‚
â”‚     â””â”€â”€â”€â”¬â”€â”€â”€â”˜   â”‚ â”€â”€â–º â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ””â”€â”€â”€â”¬â”€â”€â”€â”˜â–ˆâ–ˆâ–ˆâ”‚
â”‚         â”‚       â”‚     â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚
â”‚ â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â• â”‚     â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â•§â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚
â”‚  (railing/bg)   â”‚     â”‚  (background)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Algorithm treats image as a graph and finds optimal cuts
to separate regions based on similarity.
```

---

## Slide 33: Recognition via Matching (2000s)

**SIFT: Scale-Invariant Feature Transform** - David Lowe, 1999

```
SIFT FEATURE MATCHING
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Image 1: Stop Sign         Image 2: Stop Sign (different view)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      ____       â”‚        â”‚    ____         â”‚
â”‚     /STOP\      â”‚        â”‚   /STOP\        â”‚
â”‚    â”‚  â—   â”‚     â”‚========â”‚==â”‚  â—   â”‚       â”‚
â”‚    â”‚    â— â”‚     â”‚========â”‚==â”‚    â— â”‚       â”‚
â”‚     \____/      â”‚========â”‚===\____/        â”‚
â”‚         â—       â”‚========â”‚=======â—         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â•²                           â•±
        â•²â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•±
         Feature correspondences

SIFT detects keypoints and creates descriptors that are
invariant to scale, rotation, and illumination changes.
```

---

## Slide 34: Face Detection

**Viola and Jones, 2001**

One of the first successful applications of machine learning to vision.

```
VIOLA-JONES FACE DETECTION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”            â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”    â”‚ FACE â”‚   â”Œâ”€â”€â”€â”€â”€â”€â” â”‚
â”‚      â”‚ FACE â”‚    â”‚  âœ“   â”‚   â”‚ FACE â”‚ â”‚
â”‚      â”‚  âœ“   â”‚    â””â”€â”€â”€â”€â”€â”€â”˜   â”‚  âœ“   â”‚ â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                      â”‚
â”‚  Uses Haar-like features and         â”‚
â”‚  cascade of classifiers for          â”‚
â”‚  real-time face detection            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Slide 35: Benchmark Datasets (2004-2007)

**Caltech 101** (2004): 101 object categories, ~40-800 images each
**PASCAL Visual Object Challenge** (2007): Object detection and classification

---

## Slide 36: The Perceptron (Frank Rosenblatt, ~1957-1958)

```
THE PERCEPTRON
â”â”â”â”â”â”â”â”â”â”â”â”â”â”

          xâ‚ â”€â”€â”€â”
                 â•²
          xâ‚‚ â”€â”€â”€â”€â—â”€â”€â”€â”€â–º y = f(Î£wáµ¢xáµ¢ + b)
                 â•±
          xâ‚ƒ â”€â”€â”€â”˜

A single-layer neural network for binary classification.
```

---

## Slide 37: Minsky and Papert, 1969

```
THE XOR PROBLEM
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

   y                    XOR Truth Table
   â”‚                    â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  â—           â—     â”‚ X â”‚ Y â”‚ F(X,Y)â”‚
   â”‚                    â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚                    â”‚ 0 â”‚ 0 â”‚   0   â”‚
   â”‚                    â”‚ 0 â”‚ 1 â”‚   1   â”‚
   â”œâ”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â—â”€â–º x  â”‚ 1 â”‚ 0 â”‚   1   â”‚
   â”‚                    â”‚ 1 â”‚ 1 â”‚   0   â”‚
   â”‚                    â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜

â— = class 1 (output = 1)
â—‹ = class 0 (output = 0)

Problem: No single line can separate the classes!
Perceptrons cannot learn XOR â†’ caused disillusionment in the field.
```

---

## Slide 38: Neocognitron (Fukushima, 1980)

```
NEOCOGNITRON ARCHITECTURE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

  Input     S1      C1      S2      C2      S3      C3    Output
    â”‚       â”‚       â”‚       â”‚       â”‚       â”‚       â”‚       â”‚
â”Œâ”€â”€â”€â”´â”€â”€â”€â” â”Œâ”€â”´â”€â”   â”Œâ”€â”´â”€â”   â”Œâ”€â”´â”€â”   â”Œâ”€â”´â”€â”   â”Œâ”€â”´â”€â”   â”Œâ”€â”´â”€â”  â”Œâ”€â”´â”€â”
â”‚       â”‚ â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚  â”‚   â”‚
â”‚ IMAGE â”‚â–ºâ”‚ S â”‚â”€â”€â–ºâ”‚ C â”‚â”€â”€â–ºâ”‚ S â”‚â”€â”€â–ºâ”‚ C â”‚â”€â”€â–ºâ”‚ S â”‚â”€â”€â–ºâ”‚ C â”‚â”€â–ºâ”‚OUTâ”‚
â”‚       â”‚ â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚  â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜  â””â”€â”€â”€â”˜
            â”‚       â”‚       â”‚       â”‚       â”‚       â”‚
            â–¼       â–¼       â–¼       â–¼       â–¼       â–¼
          Simple  Complex Simple  Complex Simple  Complex
          cells   cells   cells   cells   cells   cells

Inspired by Hubel & Wiesel's findings:
- S-cells (Simple): Feature detection (like convolution)
- C-cells (Complex): Pooling for translation invariance

Problem: No practical training algorithm at the time!
```

---

## Slide 39: Backpropagation (Rumelhart, Hinton, Williams, 1986)

```
BACKPROPAGATION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

                  âˆ‚Eâ‚š     âˆ‚Eâ‚š   âˆ‚oâ‚šâ±¼
Forward Pass:     â”€â”€â”€â”€ = â”€â”€â”€â”€ Â· â”€â”€â”€â”€
                  âˆ‚wâ±¼áµ¢    âˆ‚oâ‚šâ±¼  âˆ‚wâ±¼áµ¢

     Input          Hidden         Output
     Layer          Layer          Layer

       â—‹              â—‹
        â•²            â•± â•²
         â•²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â•²
        â•± â•²        â•± â•²   â•²
       â—‹   â•²â”€â”€â”€â”€â”€â”€â—   â•²â”€â”€â”€â—â”€â”€â–º Error Eâ‚š
        â•²   â•²    â•± â•²   â•² â•±
         â•²â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â•²â”€â”€â•³
        â•±       â•±     â•²
       â—‹       â•±       â•²
              â—‹         â—‹

    â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       Gradients flow backward

Key insight: Use chain rule to compute gradients of loss
with respect to all weights, enabling training of
multi-layer networks.
```

---

## Slide 40: Convolutional Networks - LeNet (LeCun et al, 1998)

```
LeNet-5 ARCHITECTURE (LeCun et al., 1998)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

         INPUT        CONVOLUTIONS          SUBSAMPLING        FULLY CONNECTED
         32x32
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                                                         â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚  â”Œâ”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”       â”‚
    â”‚         â”‚     â”‚  â”‚C1  â”‚    â”‚S2  â”‚    â”‚C3  â”‚    â”‚S4  â”‚    â”‚C5  â”‚       â”‚
    â”‚    K    â”‚â”€â”€â”€â”€â–ºâ”‚  â”‚    â”‚â”€â”€â”€â–ºâ”‚    â”‚â”€â”€â”€â–ºâ”‚    â”‚â”€â”€â”€â–ºâ”‚    â”‚â”€â”€â”€â–ºâ”‚    â”‚â”€â”€â”    â”‚
    â”‚         â”‚     â”‚  â”‚6@  â”‚    â”‚6@  â”‚    â”‚16@ â”‚    â”‚16@ â”‚    â”‚120 â”‚  â”‚    â”‚
    â”‚ (input) â”‚     â”‚  â”‚28x â”‚    â”‚14x â”‚    â”‚10x â”‚    â”‚5x  â”‚    â”‚    â”‚  â”‚    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚28  â”‚    â”‚14  â”‚    â”‚10  â”‚    â”‚5   â”‚    â”‚    â”‚  â”‚    â”‚
                    â”‚  â””â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”˜  â”‚    â”‚
                    â”‚     â”‚          â”‚         â”‚          â”‚         â”‚  â”‚    â”‚
                    â”‚     â–¼          â–¼         â–¼          â–¼         â–¼  â”‚    â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”â”‚    â”‚
                    â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚   â”‚â–‘â–‘â–‘â–‘â–‘â”‚   â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚   â”‚â–‘â–‘â–‘â–‘â–‘â”‚   â”‚â–ˆâ–ˆâ–ˆâ–ˆâ”‚â”‚    â”‚
                    â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚   â”‚â–‘â–‘â–‘â–‘â–‘â”‚   â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚   â”‚â–‘â–‘â–‘â–‘â–‘â”‚   â”‚    â”‚â”‚    â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”˜â”‚    â”‚
                    â”‚  CONV      POOL      CONV      POOL      CONV  â”‚    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
                                                                       â”‚    â”‚
                                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
                                                              â”‚             â”‚
                                                              â–¼             â”‚
                                                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
                                                         â”‚   F6   â”‚        â”‚
                                                         â”‚   84   â”‚        â”‚
                                                         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜        â”‚
                                                              â”‚            â”‚
                                                              â–¼            â”‚
                                                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
                                                         â”‚ OUTPUT â”‚       â”‚
                                                         â”‚   10   â”‚â—„â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚(digits)â”‚
                                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    LAYER DETAILS:
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    INPUT:  32Ã—32 grayscale image
    C1:     6 feature maps, 5Ã—5 kernels â†’ 28Ã—28
    S2:     Subsampling (avg pool) 2Ã—2 â†’ 14Ã—14
    C3:     16 feature maps, 5Ã—5 kernels â†’ 10Ã—10
    S4:     Subsampling 2Ã—2 â†’ 5Ã—5
    C5:     120 feature maps, 5Ã—5 kernels â†’ 1Ã—1
    F6:     84 fully connected neurons
    OUTPUT: 10 classes (digits 0-9)

Applied backprop to Neocognitron-like architecture.
Deployed commercially by NEC for handwritten check processing.
Very similar to modern convolutional networks!
```

---

## Slides 41-42: 2000s "Deep Learning"

Research by:
- Hinton and Salakhutdinov, 2006
- Bengio et al, 2007
- Lee et al, 2009
- Glorot and Bengio, 2010

People tried to train deeper neural networks.
Not mainstream research at this time.
**No good dataset to work on.**

---

## Slide 43: ImageNet (Deng et al, 2009)

```
ImageNet Large Scale Visual Recognition Challenge
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                â”‚
â”‚   1,000 object classes                         â”‚
â”‚   1,431,167 images                             â”‚
â”‚                                                â”‚
â”‚   Challenge: Given an image, output top-5      â”‚
â”‚   most likely class labels                     â”‚
â”‚                                                â”‚
â”‚   Example output:                              â”‚
â”‚   â€¢ Scale                                      â”‚
â”‚   â€¢ T-shirt                                    â”‚
â”‚   â€¢ Steel drum  â† (correct)                    â”‚
â”‚   â€¢ Drumstick                                  â”‚
â”‚   â€¢ Mud turtle                                 â”‚
â”‚                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Slides 44-45: ImageNet Challenge Results

```
IMAGENET CLASSIFICATION ERROR RATES (%)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

30 â”‚ â–“â–“â–“ 28.2
   â”‚ â–“â–“â–“
25 â”‚ â–“â–“â–“  â–“â–“â–“ 25.8
   â”‚ â–“â–“â–“  â–“â–“â–“
20 â”‚ â–“â–“â–“  â–“â–“â–“
   â”‚ â–“â–“â–“  â–“â–“â–“  â–“â–“â–“ 16.4  â† AlexNet (2012)
15 â”‚ â–“â–“â–“  â–“â–“â–“  â–“â–“â–“
   â”‚ â–“â–“â–“  â–“â–“â–“  â–“â–“â–“  â–“â–“â–“ 11.7
10 â”‚ â–“â–“â–“  â–“â–“â–“  â–“â–“â–“  â–“â–“â–“
   â”‚ â–“â–“â–“  â–“â–“â–“  â–“â–“â–“  â–“â–“â–“  â–“â–“â–“ 7.3  â–“â–“â–“ 6.7
 5 â”‚ â–“â–“â–“  â–“â–“â–“  â–“â–“â–“  â–“â–“â–“  â–“â–“â–“ â–“â–“â–“  â–“â–“â–“ â–“â–“â–“ 3.6  â–“â–“â–“ 3.0  â–“â–“â–“ 2.3     5.1
   â”‚ â–“â–“â–“  â–“â–“â–“  â–“â–“â–“  â–“â–“â–“  â–“â–“â–“ â–“â–“â–“  â–“â–“â–“ â–“â–“â–“ â–“â–“â–“  â–“â–“â–“ â–“â–“â–“  â–“â–“â–“ â–“â–“â–“      â–“â–“â–“
 0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   2010 2011 2012 2013 2014 2014 2015 2016 2017 Human

   Lin  S&P  AlexNet Z&F  VGG  GoogLe ResNet Shao SENet  Human
                                  Net         et al
```

---

## Slides 46-47: AlexNet - Deep Learning Goes Mainstream

**Krizhevsky, Sutskever, and Hinton, NeurIPS 2012**

```
AlexNet ARCHITECTURE (2012)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Input: 224Ã—224Ã—3 RGB Image

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CONV1  â”‚    â”‚  CONV2  â”‚    â”‚  CONV3  â”‚    â”‚  CONV4  â”‚    â”‚  CONV5  â”‚
â”‚ 96@55Ã—55â”‚â”€â”€â”€â–ºâ”‚256@27Ã—27â”‚â”€â”€â”€â–ºâ”‚384@13Ã—13â”‚â”€â”€â”€â–ºâ”‚384@13Ã—13â”‚â”€â”€â”€â–ºâ”‚256@13Ã—13â”‚
â”‚ 11Ã—11,s4â”‚    â”‚ 5Ã—5     â”‚    â”‚ 3Ã—3     â”‚    â”‚ 3Ã—3     â”‚    â”‚ 3Ã—3     â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚              â”‚                                             â”‚
     â–¼              â–¼                                             â–¼
 MaxPool        MaxPool                                       MaxPool
  3Ã—3,s2         3Ã—3,s2                                        3Ã—3,s2
     â”‚              â”‚                                             â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚       FC: 4096 neurons      â”‚
                    â”‚       FC: 4096 neurons      â”‚
                    â”‚       FC: 1000 (softmax)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key innovations:
â€¢ ReLU activation (faster training)
â€¢ Dropout (regularization)
â€¢ Data augmentation
â€¢ GPU training (2 GPUs)
â€¢ ~60 million parameters

COMPARISON: AlexNet vs Neocognitron (32 years apart!)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Neocognitron (1980)              AlexNet (2012)
â€¢ Conceptually similar           â€¢ Same basic architecture
â€¢ No training algorithm          â€¢ Trained with backprop
â€¢ Small scale                    â€¢ Large scale (ImageNet)
â€¢ Theoretical                    â€¢ Practical breakthrough
```

---

## Slides 48-57: 2012 to Present - Deep Learning is Everywhere

### Applications:
- **Image Classification** - Categorizing images
- **Image Retrieval** - Finding similar images
- **Object Detection** - Locating objects with bounding boxes (Faster R-CNN)
- **Image Segmentation** - Pixel-wise classification
- **Video Classification** - Two-stream ConvNets
- **Activity Recognition** - Understanding human actions
- **Pose Estimation** - Detecting body keypoints
- **Playing Atari Games** - Deep Q-Learning
- **Medical Imaging** - Disease detection
- **Galaxy Classification** - Astronomical image analysis
- **Whale Recognition** - Wildlife identification
- **Image Captioning** - Generating text descriptions
- **Visual Relationships** - Understanding object interactions
- **Neural Style Transfer** - Artistic rendering
- **Face Generation** - GANs (Progressive Growing)
- **Text-to-Image** - DALL-E (2021)

---

## Slide 61: The Three Pillars of Deep Learning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                      â”‚
â”‚    COMPUTATION          ALGORITHMS              DATA                 â”‚
â”‚                                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚   â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚        â”‚    â—     â”‚         â”‚ â–“â–“â–“â–“â–“â–“â–“â–“ â”‚             â”‚
â”‚   â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚        â”‚   â•±â”‚â•²    â”‚         â”‚ â–“â–“â–“â–“â–“â–“â–“â–“ â”‚             â”‚
â”‚   â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚        â”‚  â— â— â—   â”‚         â”‚ Images   â”‚             â”‚
â”‚   â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚        â”‚  â”‚â•²â”‚â•±â”‚   â”‚         â”‚ â–“â–“â–“â–“â–“â–“â–“â–“ â”‚             â”‚
â”‚   â”‚ GPUs     â”‚        â”‚  â—â”€â—â”€â—   â”‚         â”‚ â–“â–“â–“â–“â–“â–“â–“â–“ â”‚             â”‚
â”‚   â”‚ TPUs     â”‚        â”‚  â”‚â•²â”‚â•±â”‚   â”‚         â”‚ â–“â–“â–“â–“â–“â–“â–“â–“ â”‚             â”‚
â”‚   â”‚ Clusters â”‚        â”‚  â— â— â—   â”‚         â”‚ â–“â–“â–“â–“â–“â–“â–“â–“ â”‚             â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                                      â”‚
â”‚   Hardware advances   Neural network        Large labeled            â”‚
â”‚   enabling massive    architectures         datasets                 â”‚
â”‚   parallel computing  and training          (ImageNet, etc.)         â”‚
â”‚                       algorithms                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Slides 62-63: GPU Performance Growth

```
GFLOP per Dollar Over Time
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

200 â”‚                                          â—‹ Tensor Core GPUs
    â”‚                                        â—‹
    â”‚                                      â—‹
150 â”‚                                    â—‹
    â”‚                                  â—‹
    â”‚                                â—‹
100 â”‚                              â—‹
    â”‚
    â”‚                          â—   â† RTX 2080 Ti (FP32)
 50 â”‚                        â—
    â”‚         Deep Learning  â—
    â”‚         Explosion â”€â”€â–ºâ—â—
  0 â”‚___â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—_____________________________
    2004    2008    2012    2016    2020

    â— = CPU (flat)
    â— = GPU FP32 (exponential growth)
    â—‹ = GPU Tensor Core (even faster!)

Recent GPUs have "Tensor Cores": Special hardware for deep learning!
```

---

## Slide 64: AI's Explosive Growth & Impact

- Conference attendance: Exponential growth (CVPR, NeurIPS, ICML, etc.)
- AI Startups: Rapid increase since 2012
- Enterprise AI Revenue: From ~$1B (2016) to ~$30B+ (2020s)

---

## Slides 65-68: Computer Vision's Future

### Despite successes, computer vision still has a long way to go

**Computer Vision Can Cause Harm:**
- Harmful stereotypes (bias in facial recognition)
- Affecting people's lives (AI hiring algorithms)
- Performance disparities across demographic groups

**Computer Vision Can Save Lives:**
- Healthcare monitoring for seniors
- Early symptom detection
- Managing chronic conditions
- Privacy-preserving approaches

**Much We Don't Know How To Do:**
- Deep scene understanding
- Common sense reasoning
- Contextual understanding of complex social situations

---

## Slide 69: Today's Agenda (Recap)

- âœ“ A brief history of computer vision & deep learning
- CS231n overview (continued in Part 2)

---

## Historical Timeline

```
COMPUTER VISION & DEEP LEARNING TIMELINE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1958    1959    1963    1969    1970s   1979    1980    1985    1986
  â”‚       â”‚       â”‚       â”‚       â”‚       â”‚       â”‚       â”‚       â”‚
  â–¼       â–¼       â–¼       â–¼       â–¼       â–¼       â–¼       â–¼       â–¼
Percep- Hubel   Larry  Minsky  David   Gen.   Neoco- Back-  Canny
tron    &       Roberts &       Marr   Cylin- gnitron prop   Edge
        Wiesel          Papert         ders                  Detect

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                        AI WINTER (late 1980s - 1990s)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1997    1998    1999    2001    2004-07  2006    2009    2012    Present
  â”‚       â”‚       â”‚       â”‚       â”‚        â”‚       â”‚       â”‚       â”‚
  â–¼       â–¼       â–¼       â–¼       â–¼        â–¼       â–¼       â–¼       â–¼
Norm.   LeNet   SIFT   Viola   Caltech  Deep   ImageNet AlexNet Deep
Cuts                   &       101,    Learn-                    Learning
                       Jones   PASCAL  ing                       Explosion

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

*Slide inspiration credits: Justin Johnson, Andrej Karpathy*

*Document extracted from CS231n Lecture 1 Part 1 PDF (69 slides)*
*Stanford University, April 2025*

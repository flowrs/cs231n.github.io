# Lecture 6: Training CNNs and CNN Architectures
## Stanford CS231n 10th Anniversary - April 17, 2025

---

## Lecture Overview

Two main topics:
1. **How to build CNNs** - Layers, activations, architectures
2. **How to train CNNs** - Initialization, normalization, data augmentation, transfer learning

---

## Activation Functions Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ACTIVATION FUNCTIONS                                â”‚
â”‚                                                                  â”‚
â”‚   SIGMOID: Ïƒ(x) = 1/(1+e^-x)                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚   â”‚         ___â”€â”€â”€â”€    â”‚   Problems:                            â”‚
â”‚   â”‚      __/          â”‚   - Saturates (kills gradients)        â”‚
â”‚   â”‚    _/              â”‚   - Not zero-centered                  â”‚
â”‚   â”‚â”€â”€â”€/                â”‚   - exp() is slow                      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   âŒ Don't use                         â”‚
â”‚                                                                  â”‚
â”‚   TANH: tanh(x)                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚   â”‚         ___â”€â”€â”€â”€    â”‚   Better:                              â”‚
â”‚   â”‚      __/          â”‚   - Zero-centered âœ“                    â”‚
â”‚   â”‚â”€â”€â”€â”€_/â”€â”€â”€â”€          â”‚   But still saturates âŒ               â”‚
â”‚   â”‚  _/                â”‚                                        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚                                                                  â”‚
â”‚   ReLU: f(x) = max(0, x)                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚   â”‚              /     â”‚   âœ“ No saturation (positive)          â”‚
â”‚   â”‚            /       â”‚   âœ“ Fast (just max)                   â”‚
â”‚   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€/         â”‚   âœ“ Converges fast                    â”‚
â”‚   â”‚        /           â”‚   âŒ Dead neurons (x < 0)             â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   âœ“ DEFAULT CHOICE                    â”‚
â”‚                                                                  â”‚
â”‚   LEAKY ReLU: f(x) = max(0.01x, x)                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚   â”‚              /     â”‚   âœ“ No dead neurons                   â”‚
â”‚   â”‚            /       â”‚   âœ“ Small negative slope              â”‚
â”‚   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€/         â”‚   Often works slightly better         â”‚
â”‚   â”‚        /           â”‚                                        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Weight Initialization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              WEIGHT INITIALIZATION                               â”‚
â”‚                                                                  â”‚
â”‚   WHY IT MATTERS:                                                â”‚
â”‚   - Too small â†’ activations shrink to 0 (vanishing)            â”‚
â”‚   - Too large â†’ activations explode (exploding)                â”‚
â”‚   - Both â†’ training fails!                                      â”‚
â”‚                                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   BAD: Small random weights                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  W = 0.01 * np.random.randn(Din, Dout)                  â”‚   â”‚
â”‚   â”‚                                                         â”‚   â”‚
â”‚   â”‚  Layer 1 â†’ Layer 2 â†’ Layer 3 â†’ ... â†’ Layer 6           â”‚   â”‚
â”‚   â”‚   [good]   [small]  [tiny]   ...   [~0 dead!]          â”‚   â”‚
â”‚   â”‚                                                         â”‚   â”‚
â”‚   â”‚  Activations collapse to zero!                         â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚   GOOD: Xavier/Glorot (for tanh/sigmoid)                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  W = np.random.randn(Din, Dout) * np.sqrt(1/Din)       â”‚   â”‚
â”‚   â”‚                                                         â”‚   â”‚
â”‚   â”‚  Idea: Keep variance ~1 through layers                 â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚   BEST: Kaiming/He (for ReLU)                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  W = np.random.randn(Din, Dout) * np.sqrt(2/Din)       â”‚   â”‚
â”‚   â”‚                                                         â”‚   â”‚
â”‚   â”‚  Factor of 2 accounts for ReLU killing half the values â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

*Image: Weight init code (images/lecture_6/page060_img97.png, page065_img105.png)*

---

## Batch Normalization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              BATCH NORMALIZATION                                 â”‚
â”‚                                                                  â”‚
â”‚   Idea: Normalize activations to have mean=0, variance=1       â”‚
â”‚   Then let network learn optimal scale and shift                â”‚
â”‚                                                                  â”‚
â”‚   ALGORITHM (per feature channel):                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                                                         â”‚   â”‚
â”‚   â”‚   1. Compute batch mean:    Î¼ = (1/N) Î£ xáµ¢             â”‚   â”‚
â”‚   â”‚                                                         â”‚   â”‚
â”‚   â”‚   2. Compute batch var:     ÏƒÂ² = (1/N) Î£ (xáµ¢ - Î¼)Â²     â”‚   â”‚
â”‚   â”‚                                                         â”‚   â”‚
â”‚   â”‚   3. Normalize:             xÌ‚áµ¢ = (xáµ¢ - Î¼) / âˆš(ÏƒÂ² + Îµ)  â”‚   â”‚
â”‚   â”‚                                                         â”‚   â”‚
â”‚   â”‚   4. Scale and shift:       yáµ¢ = Î³ Â· xÌ‚áµ¢ + Î²            â”‚   â”‚
â”‚   â”‚                             (learned parameters)        â”‚   â”‚
â”‚   â”‚                                                         â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚   BENEFITS:                                                      â”‚
â”‚   âœ“ Faster training (higher learning rates)                    â”‚
â”‚   âœ“ Less sensitive to initialization                           â”‚
â”‚   âœ“ Acts as regularization (reduces need for dropout)          â”‚
â”‚   âœ“ Makes very deep networks trainable                         â”‚
â”‚                                                                  â”‚
â”‚   PLACEMENT: Usually after linear/conv, before activation       â”‚
â”‚   Conv â†’ BatchNorm â†’ ReLU â†’ Conv â†’ BatchNorm â†’ ReLU â†’ ...      â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Types of Normalization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              NORMALIZATION VARIANTS                              â”‚
â”‚                                                                  â”‚
â”‚   Tensor shape: [N, C, H, W] (batch, channels, height, width)  â”‚
â”‚                                                                  â”‚
â”‚   BATCH NORM              LAYER NORM                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚   â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚         â”‚ â–ˆ           â”‚                       â”‚
â”‚   â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚         â”‚ â–ˆ           â”‚                       â”‚
â”‚   â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚         â”‚ â–ˆ           â”‚                       â”‚
â”‚   â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚         â”‚ â–ˆ           â”‚                       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚   Normalize across        Normalize across                      â”‚
â”‚   batch (N) for each      channels (C,H,W) for                 â”‚
â”‚   channel                 each sample                           â”‚
â”‚   â†’ CNNs (default)        â†’ Transformers, RNNs                  â”‚
â”‚                                                                  â”‚
â”‚   INSTANCE NORM           GROUP NORM                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚   â”‚ â–ˆ           â”‚         â”‚ â–ˆâ–ˆ          â”‚                       â”‚
â”‚   â”‚             â”‚         â”‚ â–ˆâ–ˆ          â”‚                       â”‚
â”‚   â”‚             â”‚         â”‚             â”‚                       â”‚
â”‚   â”‚             â”‚         â”‚             â”‚                       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚   Normalize per           Normalize groups                      â”‚
â”‚   sample, per channel     of channels                          â”‚
â”‚   â†’ Style transfer        â†’ When batch size is small           â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

*Image: Normalization comparison (images/lecture_6/page012_img08.png)*

---

## Data Augmentation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DATA AUGMENTATION                                   â”‚
â”‚                                                                  â”‚
â”‚   Create more training data by applying transformations!        â”‚
â”‚                                                                  â”‚
â”‚   ORIGINAL          AUGMENTED VERSIONS                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚   â”‚         â”‚      â”‚         â”‚ â”‚  â—„â”€â”€â”€   â”‚ â”‚   â–“â–“â–“   â”‚         â”‚
â”‚   â”‚   ğŸ±    â”‚  â†’   â”‚   ğŸ±    â”‚ â”‚   ğŸ±    â”‚ â”‚   ğŸ±    â”‚         â”‚
â”‚   â”‚         â”‚      â”‚  (crop) â”‚ â”‚  (flip) â”‚ â”‚ (color) â”‚         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                  â”‚
â”‚   COMMON AUGMENTATIONS:                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                                                         â”‚   â”‚
â”‚   â”‚   â€¢ Random cropping       â€¢ Horizontal flip             â”‚   â”‚
â”‚   â”‚   â€¢ Random scaling        â€¢ Color jitter                â”‚   â”‚
â”‚   â”‚   â€¢ Random rotation       â€¢ Cutout/Random erasing       â”‚   â”‚
â”‚   â”‚   â€¢ Mixup (blend images)  â€¢ CutMix (paste patches)     â”‚   â”‚
â”‚   â”‚   â€¢ AutoAugment (learned) â€¢ RandAugment                â”‚   â”‚
â”‚   â”‚                                                         â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚   Apply ONLY during training (not test/inference)               â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## CNN Architecture: VGGNet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              VGGNet (2014)                                       â”‚
â”‚                                                                  â”‚
â”‚   Key idea: Use SMALL filters (3Ã—3) but go DEEPER               â”‚
â”‚                                                                  â”‚
â”‚   INPUT: 224Ã—224Ã—3                                              â”‚
â”‚                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ CONV3-64  â”€â”€â–º CONV3-64  â”€â”€â–º MAXPOOL                     â”‚   â”‚
â”‚   â”‚ CONV3-128 â”€â”€â–º CONV3-128 â”€â”€â–º MAXPOOL                     â”‚   â”‚
â”‚   â”‚ CONV3-256 â”€â”€â–º CONV3-256 â”€â”€â–º CONV3-256 â”€â”€â–º MAXPOOL       â”‚   â”‚
â”‚   â”‚ CONV3-512 â”€â”€â–º CONV3-512 â”€â”€â–º CONV3-512 â”€â”€â–º MAXPOOL       â”‚   â”‚
â”‚   â”‚ CONV3-512 â”€â”€â–º CONV3-512 â”€â”€â–º CONV3-512 â”€â”€â–º MAXPOOL       â”‚   â”‚
â”‚   â”‚                                                         â”‚   â”‚
â”‚   â”‚ FC-4096 â”€â”€â–º FC-4096 â”€â”€â–º FC-1000 â”€â”€â–º Softmax            â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚   VGG-16: 16 weight layers, 138M parameters                     â”‚
â”‚   VGG-19: 19 weight layers                                      â”‚
â”‚                                                                  â”‚
â”‚   WHY 3Ã—3 FILTERS?                                               â”‚
â”‚   Two 3Ã—3 conv = one 5Ã—5 receptive field                        â”‚
â”‚   Three 3Ã—3 conv = one 7Ã—7 receptive field                      â”‚
â”‚   But with MORE non-linearities and FEWER parameters!           â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## CNN Architecture: ResNet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ResNet (2015) - Residual Networks                   â”‚
â”‚                                                                  â”‚
â”‚   Problem: Very deep networks are hard to train                 â”‚
â”‚   Solution: SKIP CONNECTIONS (residual learning)                â”‚
â”‚                                                                  â”‚
â”‚   RESIDUAL BLOCK:                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                                                         â”‚   â”‚
â”‚   â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚   â”‚
â”‚   â”‚          â”‚                    â”‚                        â”‚   â”‚
â”‚   â”‚    x â”€â”€â”€â”€â”¤                    â”œâ”€â”€â”€â–º (+) â”€â”€â–º F(x) + x   â”‚   â”‚
â”‚   â”‚          â”‚   Weight layers    â”‚      â†‘                 â”‚   â”‚
â”‚   â”‚          â”‚   (Conv-BN-ReLU    â”‚      â”‚                 â”‚   â”‚
â”‚   â”‚          â”‚    Conv-BN)        â”‚      â”‚                 â”‚   â”‚
â”‚   â”‚          â”‚         = F(x)     â”‚      â”‚                 â”‚   â”‚
â”‚   â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚                 â”‚   â”‚
â”‚   â”‚                    â”‚                 â”‚                 â”‚   â”‚
â”‚   â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚   â”‚
â”‚   â”‚                    identity shortcut                   â”‚   â”‚
â”‚   â”‚                                                         â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚   WHY IT WORKS:                                                  â”‚
â”‚   - If F(x) should be identity, just learn F(x) = 0            â”‚
â”‚   - Gradients can flow directly through shortcut               â”‚
â”‚   - Enables training 100+ layer networks!                      â”‚
â”‚                                                                  â”‚
â”‚   VARIANTS: ResNet-18, ResNet-34, ResNet-50, ResNet-101, -152  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Transfer Learning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              TRANSFER LEARNING                                   â”‚
â”‚                                                                  â”‚
â”‚   Use features learned on one task for another task!            â”‚
â”‚                                                                  â”‚
â”‚   PRE-TRAINED MODEL (e.g., ImageNet):                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ Conv1 â”‚ Conv2 â”‚ Conv3 â”‚ Conv4 â”‚ Conv5 â”‚  FC  â”‚ FC-1000 â”‚   â”‚
â”‚   â”‚       â”‚       â”‚       â”‚       â”‚       â”‚      â”‚ classes â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚     Low-level      Mid-level      High-level    Task-specific   â”‚
â”‚     (edges)        (textures)     (parts)       (ImageNet)      â”‚
â”‚                                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   STRATEGY 1: Feature Extraction (small dataset)                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ Conv1 â”‚ Conv2 â”‚ Conv3 â”‚ Conv4 â”‚ Conv5 â”‚  FC  â”‚ FC-NEW  â”‚   â”‚
â”‚   â”‚ FROZENâ”‚FROZEN â”‚FROZEN â”‚FROZEN â”‚FROZEN â”‚      â”‚ classes â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚   Only train the new FC layer                                   â”‚
â”‚                                                                  â”‚
â”‚   STRATEGY 2: Fine-tuning (more data)                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ Conv1 â”‚ Conv2 â”‚ Conv3 â”‚ Conv4 â”‚ Conv5 â”‚  FC  â”‚ FC-NEW  â”‚   â”‚
â”‚   â”‚ FROZENâ”‚FROZEN â”‚FROZEN â”‚ TRAIN â”‚ TRAIN â”‚TRAIN â”‚ TRAIN   â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚   Train later layers with small learning rate                   â”‚
â”‚                                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   RULE OF THUMB:                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  Your data       â”‚  Strategy                             â”‚  â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚   â”‚  Small, similar  â”‚  Feature extraction (freeze all)     â”‚  â”‚
â”‚   â”‚  Small, differentâ”‚  Feature extraction from earlier     â”‚  â”‚
â”‚   â”‚  Large, similar  â”‚  Fine-tune all layers                â”‚  â”‚
â”‚   â”‚  Large, differentâ”‚  Train from scratch or fine-tune     â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KEY TAKEAWAYS                                 â”‚
â”‚                                                                  â”‚
â”‚   BUILDING CNNs:                                                 â”‚
â”‚   â€¢ Use ReLU activation (or Leaky ReLU)                         â”‚
â”‚   â€¢ Use 3Ã—3 convolutions, stack them deep                       â”‚
â”‚   â€¢ Use BatchNorm after conv/linear layers                      â”‚
â”‚   â€¢ Use residual connections for very deep networks             â”‚
â”‚                                                                  â”‚
â”‚   TRAINING CNNs:                                                 â”‚
â”‚   â€¢ Use Kaiming initialization for ReLU networks                â”‚
â”‚   â€¢ Apply data augmentation (flips, crops, color)               â”‚
â”‚   â€¢ Use transfer learning when you have limited data            â”‚
â”‚   â€¢ Monitor training/validation loss for overfitting            â”‚
â”‚                                                                  â”‚
â”‚   FAMOUS ARCHITECTURES:                                          â”‚
â”‚   â€¢ AlexNet (2012): First deep CNN winner                       â”‚
â”‚   â€¢ VGGNet (2014): Simple, deep, 3Ã—3 convs                      â”‚
â”‚   â€¢ GoogLeNet (2014): Inception modules                         â”‚
â”‚   â€¢ ResNet (2015): Skip connections, very deep                  â”‚
â”‚   â€¢ DenseNet (2017): Dense connections                          â”‚
â”‚   â€¢ EfficientNet (2019): Neural architecture search             â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Image References

Key images from: `images/lecture_6/`

- `page012_img08.png` - Normalization variants comparison
- `page060_img97.png` - Small weight initialization code
- `page065_img105.png` - Kaiming initialization code
